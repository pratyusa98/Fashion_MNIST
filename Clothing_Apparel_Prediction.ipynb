{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 4us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 36s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 5s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train) , (X_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(X_train[4])\n",
    "y_train[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples/observations in training data: 60000\n",
      "Number of labels in training data: 60000\n",
      "Dimensions of a single image in x_train:(28, 28)\n",
      "-------------------------------------------------------------\n",
      "Number of samples/observations in test data: 10000\n",
      "Number of labels in test data: 10000\n",
      "Dimensions of single image in x_test:(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "# Check the shape and size of x_train, x_test, y_train, y_test\n",
    "print (\"Number of samples/observations in training data: \" + str(len(X_train)))\n",
    "print (\"Number of labels in training data: \" + str(len(y_train)))\n",
    "print (\"Dimensions of a single image in x_train:\" + str(X_train[0].shape))\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print (\"Number of samples/observations in test data: \" + str(len(X_test)))\n",
    "print (\"Number of labels in test data: \" + str(len(y_test)))\n",
    "print (\"Dimensions of single image in x_test:\" + str(X_test[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary keras specific libraries\n",
    "\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the number of rows and columns\n",
    "img_rows = X_train[0].shape[0]\n",
    "img_cols = X_train[1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Getting the data in the right 'shape' as required by Keras i.e. adding a 4th \n",
    "dimension to our data thereby changing the original image shape of (60000,28,28) \n",
    "to (60000,28,28,1)'''\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the shape of a single image \n",
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing image type to float32 data type\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data by changing the image pixel range from (0 to 255) to (0 to 1)\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of classes and number of pixels \n",
    "# num_classes = y_test.shape[1]\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,200,778\n",
      "Trainable params: 1,200,330\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create CNN model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              optimizer = keras.optimizers.Adam(),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 341s 182ms/step - loss: 0.4110 - accuracy: 0.8581 - val_loss: 0.3014 - val_accuracy: 0.8866\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 340s 181ms/step - loss: 0.2844 - accuracy: 0.8985 - val_loss: 0.2517 - val_accuracy: 0.9075\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 386s 206ms/step - loss: 0.2460 - accuracy: 0.9132 - val_loss: 0.2502 - val_accuracy: 0.9124\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 400s 214ms/step - loss: 0.2171 - accuracy: 0.9227 - val_loss: 0.2344 - val_accuracy: 0.9138\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 411s 219ms/step - loss: 0.1998 - accuracy: 0.9276 - val_loss: 0.2317 - val_accuracy: 0.9165\n",
      "Test loss: 0.23172087967395782\n",
      "Test accuracy: 0.9164999723434448\n"
     ]
    }
   ],
   "source": [
    "model_fitting = model.fit(X_train, y_train,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model with the name clothing_classification_model\n",
    "model.save('clothing_classification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4 - Making a single prediction\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "def predict_img(img):\n",
    "\n",
    "    test_image = image.load_img(img, color_mode = \"grayscale\",target_size = (28,28))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = test_image.reshape(1, 28, 28, 1)\n",
    "    test_image = test_image.astype('float32')\n",
    "    test_image=test_image/255\n",
    "    result = model.predict(test_image)\n",
    "    class_prediction = np.argmax(result)\n",
    "    \n",
    "    #Map apparel category with the numerical class\n",
    "    if class_prediction == 0:\n",
    "      product = \"T-shirt/top\"\n",
    "    elif class_prediction == 1:\n",
    "      product = \"Trouser\"\n",
    "    elif class_prediction == 2:\n",
    "      product = \"Pullover\"\n",
    "    elif class_prediction == 3:\n",
    "      product = \"Dress\"\n",
    "    elif class_prediction == 4:\n",
    "      product = \"Coat\"\n",
    "    elif class_prediction == 5:\n",
    "      product = \"Sandal\"\n",
    "    elif class_prediction == 6:\n",
    "      product = \"Shirt\"\n",
    "    elif class_prediction == 7:\n",
    "      product = \"Sneaker\"\n",
    "    elif class_prediction == 8:\n",
    "      product = \"Bag\"\n",
    "    else:\n",
    "      product = \"Ankle boot\"\n",
    "    \n",
    "    return product\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bag'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_img('bag.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
